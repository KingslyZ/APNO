{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Concatenate, Multiply\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('./changhua_data.csv')\n",
    "\n",
    "# Extract columns\n",
    "# Assume: First column is time, second column is flow, third to ninth are rainfall data\n",
    "times = pd.to_datetime(data.iloc[:, 0].values)  # Convert time to datetime\n",
    "flows = data.iloc[:, 1].values.reshape(-1, 1)\n",
    "rainfalls = data.iloc[:, 2:].values\n",
    "print(\"rainfall shape\",rainfalls.shape)\n",
    "\n",
    "# Create sequences for time-series prediction\n",
    "def create_sequences(data, target, input_steps, output_steps):\n",
    "    X, y, times_seq = [], [], []\n",
    "    for i in range(len(data) - input_steps - output_steps + 1):\n",
    "        X.append(data[i:(i + input_steps), :])\n",
    "        y.append(target[(i + input_steps):(i + input_steps + output_steps), 0])\n",
    "        times_seq.append(times[i + input_steps:(i + input_steps + output_steps)])\n",
    "    return np.array(X), np.array(y), np.array(times_seq)\n",
    "\n",
    "input_steps = 12  # Use past 12 hours\n",
    "output_steps = 6  # Predict next 6 hours\n",
    "\n",
    "# Data preprocessing\n",
    "scaler_flow = MinMaxScaler()\n",
    "flows_scaled = scaler_flow.fit_transform(flows)\n",
    "\n",
    "scaler_rainfall = MinMaxScaler()\n",
    "rainfalls_scaled = scaler_rainfall.fit_transform(rainfalls)\n",
    "\n",
    "# Combine rainfall and flow data for sequence creation\n",
    "combined_data = np.concatenate((rainfalls_scaled, flows_scaled), axis=1)\n",
    "\n",
    "# Create sequences\n",
    "X, y, times_seq = create_sequences(combined_data, flows_scaled, input_steps, output_steps)\n",
    "\n",
    "# Train-test split 不按照时间顺序分割\n",
    "# X_train, X_test, y_train, y_test, times_train, times_test = train_test_split(X, y, times_seq, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train-test split using time-based split to maintain sequence continuity 按照时间顺序分割\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "times_train, times_test = times_seq[:split_index], times_seq[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepONet implementation\n",
    "# Branch Network: U-Net with CBAM\n",
    "from tensorflow.keras.layers import Conv1D, UpSampling1D, MaxPooling1D, concatenate, GlobalAveragePooling1D, GlobalMaxPooling1D, Reshape, Dense, multiply, add, Activation\n",
    "\n",
    "# Convolutional Block Attention Module (CBAM)\n",
    "def cbam_block(input_feature, ratio=8):\n",
    "    # Channel Attention Module\n",
    "    channel = input_feature.shape[-1]\n",
    "    shared_layer_one = Dense(channel // ratio, activation='relu')\n",
    "    shared_layer_two = Dense(channel)\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(input_feature)\n",
    "    avg_pool = Reshape((1, channel))(avg_pool)\n",
    "    avg_pool = shared_layer_one(avg_pool)\n",
    "    avg_pool = shared_layer_two(avg_pool)\n",
    "    \n",
    "    max_pool = GlobalMaxPooling1D()(input_feature)\n",
    "    max_pool = Reshape((1, channel))(max_pool)\n",
    "    max_pool = shared_layer_one(max_pool)\n",
    "    max_pool = shared_layer_two(max_pool)\n",
    "    \n",
    "    cbam_feature = add([avg_pool, max_pool])\n",
    "    cbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "    cbam_feature = multiply([input_feature, cbam_feature])\n",
    "    \n",
    "    # Spatial Attention Module\n",
    "    avg_pool = tf.reduce_mean(cbam_feature, axis=-1, keepdims=True)\n",
    "    max_pool = tf.reduce_max(cbam_feature, axis=-1, keepdims=True)\n",
    "    concat = concatenate([avg_pool, max_pool], axis=-1)\n",
    "    cbam_feature = Conv1D(filters=1, kernel_size=7, padding='same', activation='sigmoid')(concat)\n",
    "    cbam_feature = multiply([cbam_feature, input_feature])\n",
    "    \n",
    "    return cbam_feature\n",
    "\n",
    "# U-Net with CBAM\n",
    "branch_input = Input(shape=(input_steps, X_train.shape[2] - 1), name='branch_input')\n",
    "conv1 = Conv1D(64, 3, activation='relu', padding='same')(branch_input)\n",
    "conv1 = cbam_block(conv1)\n",
    "pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "\n",
    "conv2 = Conv1D(128, 3, activation='relu', padding='same')(pool1)\n",
    "conv2 = cbam_block(conv2)\n",
    "pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "\n",
    "conv3 = Conv1D(256, 3, activation='relu', padding='same')(pool2)\n",
    "conv3 = cbam_block(conv3)\n",
    "\n",
    "up4 = UpSampling1D(size=2)(conv3)\n",
    "up4 = concatenate([up4, conv2], axis=-1)\n",
    "conv4 = Conv1D(128, 3, activation='relu', padding='same')(up4)\n",
    "conv4 = cbam_block(conv4)\n",
    "\n",
    "up5 = UpSampling1D(size=2)(conv4)\n",
    "up5 = concatenate([up5, conv1], axis=-1)\n",
    "conv5 = Conv1D(64, 3, activation='relu', padding='same')(up5)\n",
    "conv5 = cbam_block(conv5)\n",
    "\n",
    "branch_output = Conv1D(32, 1, activation='relu')(conv5)\n",
    "\n",
    "# Trunk Network: Takes the flow data as input\n",
    "# trunk_input = Input(shape=(input_steps, 1), name='trunk_input')\n",
    "# trunk_lstm = LSTM(64, activation='relu')(trunk_input)\n",
    "# trunk_output = Dense(32, activation='relu')(trunk_lstm)\n",
    "\n",
    "# Trunk Network: Replace LSTM with FNN (Fully Connected Neural Network)\n",
    "trunk_input = Input(shape=(input_steps, 1), name='trunk_input')\n",
    "flatten_input = tf.keras.layers.Flatten()(trunk_input)\n",
    "trunk_dense_1 = Dense(64, activation='relu')(flatten_input)\n",
    "trunk_dense_2 = Dense(32, activation='relu')(trunk_dense_1)\n",
    "trunk_output = trunk_dense_2\n",
    "\n",
    "# Replace pointwise multiplication with FNO and GRU layers\n",
    "# Fourier Neural Operator (FNO) layer\n",
    "from tensorflow.keras.layers import Conv1D, LayerNormalization\n",
    "\n",
    "fno_layer_1 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(branch_output[:, None, :])\n",
    "fno_layer_2 = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(fno_layer_1)\n",
    "fno_layer_2 = tf.squeeze(fno_layer_2, axis=1)\n",
    "\n",
    "# GRU layers\n",
    "gru_layer_1 = LSTM(32, return_sequences=True, activation='relu')(fno_layer_2)\n",
    "gru_layer_2 = LSTM(32, activation='relu')(gru_layer_1)\n",
    "\n",
    "# Combine FNO and GRU output\n",
    "combined = Multiply()([trunk_output, gru_layer_2])\n",
    "combined_output = Dense(output_steps, activation='linear')(combined)\n",
    "\n",
    "# Physics-Informed Neural Networks (PINN) Loss\n",
    "# Custom loss function that incorporates both data loss and physical loss\n",
    "\n",
    "def pinn_loss(y_true, y_pred):\n",
    "    # Initialize self-adaptive weights as trainable variables\n",
    "    if not hasattr(pinn_loss, 'data_weight'):\n",
    "        pinn_loss.data_weight = tf.Variable(0.7, trainable=True, dtype=tf.float32, name='data_weight')\n",
    "    if not hasattr(pinn_loss, 'physics_weight'):\n",
    "        pinn_loss.physics_weight = tf.Variable(0.3, trainable=True, dtype=tf.float32, name='physics_weight')\n",
    "\n",
    "    # Mean squared error loss for data fitting\n",
    "    data_loss = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    \n",
    "    # Physics loss: based on Saint-Venant equations residuals\n",
    "    g = 9.81  # gravitational acceleration (m/s^2)\n",
    "    physics_loss = 0\n",
    "    for i in range(1, output_steps):\n",
    "        # Approximate depth (h) and velocity (u) based on predictions\n",
    "        h = y_pred[:, i]\n",
    "        h_prev = y_pred[:, i - 1]\n",
    "        \n",
    "        # Mass conservation residual (continuity equation)\n",
    "        dh_dt = (h - h_prev) / (1.0)  # Simplified time derivative (assuming unit time step)\n",
    "        physics_mass_residual = dh_dt  # In reality, add spatial flow divergence terms\n",
    "        \n",
    "        # Momentum conservation residual (momentum equation)\n",
    "        du_dt = (h - h_prev) / (1.0)  # Simplified time derivative for velocity\n",
    "        slope_term = -g * (h - h_prev) / (1.0)  # Simplified slope term\n",
    "        physics_momentum_residual = du_dt + slope_term\n",
    "        \n",
    "        # Combine physics residuals\n",
    "        physics_loss += tf.reduce_mean(tf.square(physics_mass_residual))\n",
    "        physics_loss += tf.reduce_mean(tf.square(physics_momentum_residual))\n",
    "    \n",
    "    # Boundary condition: assume known boundary condition at the beginning and end of prediction\n",
    "    boundary_condition_loss = tf.reduce_mean(tf.square(y_pred[:, 0] - y_true[:, 0]))\n",
    "    boundary_condition_loss += tf.reduce_mean(tf.square(y_pred[:, -1] - y_true[:, -1]))\n",
    "    \n",
    "    # Initial condition loss: make sure the initial state is consistent\n",
    "    initial_condition_loss = tf.reduce_mean(tf.square(y_pred[:, 0] - y_true[:, 0]))\n",
    "    \n",
    "    # Combine physics losses\n",
    "    total_physics_loss = physics_loss + boundary_condition_loss + initial_condition_loss\n",
    "    \n",
    "    # Compute gradients for GradNorm\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch([pinn_loss.data_weight, pinn_loss.physics_weight])\n",
    "        weighted_total_loss = pinn_loss.data_weight * data_loss + pinn_loss.physics_weight * total_physics_loss\n",
    "    grads = tape.gradient(weighted_total_loss, [pinn_loss.data_weight, pinn_loss.physics_weight])\n",
    "    grad_data, grad_physics = grads[0], grads[1]\n",
    "\n",
    "    # Compute GradNorm adjustment\n",
    "    target_ratio = 1.0  # Define target ratio for balancing gradients\n",
    "    grad_norm_ratio = tf.clip_by_value(tf.abs(grad_data) / (tf.abs(grad_physics) + 1e-2), 0.1, 10.0)  # Clip to prevent extreme values\n",
    "    grad_adjustment = tf.clip_by_value(target_ratio / (grad_norm_ratio + 1e-2), 0.1, 10.0)  # Clip adjustment to stabilize training\n",
    "\n",
    "    # Adjust self-adaptive weights using GradNorm\n",
    "    # pinn_loss.data_weight.assign(pinn_loss.data_weight * grad_adjustment)\n",
    "    # pinn_loss.physics_weight.assign(pinn_loss.physics_weight / grad_adjustment)\n",
    "\n",
    "    # Adjust self-adaptive weights using GradNorm with a learning rate\n",
    "    adjustment_lr = 0.01  # Learning rate for adjusting weights\n",
    "    new_data_weight = pinn_loss.data_weight * (1.0 + adjustment_lr * (grad_adjustment - 1.0))\n",
    "    new_physics_weight = pinn_loss.physics_weight * (1.0 - adjustment_lr * (grad_adjustment - 1.0))\n",
    "\n",
    "    pinn_loss.data_weight.assign(tf.clip_by_value(new_data_weight, 0.1, 10.0))\n",
    "    pinn_loss.physics_weight.assign(tf.clip_by_value(new_physics_weight, 0.1, 10.0))\n",
    "    pinn_loss.data_weight.assign(pinn_loss.data_weight * grad_adjustment)\n",
    "    pinn_loss.physics_weight.assign(pinn_loss.physics_weight / grad_adjustment)\n",
    "\n",
    "\n",
    "    # Combine all losses\n",
    "    total_loss = pinn_loss.data_weight * data_loss + pinn_loss.physics_weight * total_physics_loss\n",
    "    return total_loss\n",
    "\n",
    "# Model definition\n",
    "model = Model(inputs=[branch_input, trunk_input], outputs=combined_output)\n",
    "model.compile(optimizer='adam', loss=pinn_loss, metrics=['mae'])\n",
    "# model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit([X_train[:, :, :-1], X_train[:, :, -1:]], y_train, epochs=10, batch_size=16, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation = model.evaluate([X_test[:, :, :-1], X_test[:, :, -1:]], y_test)\n",
    "print(f'Test Loss: {evaluation[0]}, Test MAE: {evaluation[1]}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict([X_test[:, :, :-1], X_test[:, :, -1:]])\n",
    "\n",
    "# Only keep the first future step predictions to avoid overlap\n",
    "predictions = predictions[:, 0].reshape(-1, 1)\n",
    "y_test = y_test[:, 0].reshape(-1, 1)\n",
    "\n",
    "# Inverse transform the predicted flows to original scale\n",
    "predicted_flows = scaler_flow.inverse_transform(predictions)\n",
    "y_test_inverse = scaler_flow.inverse_transform(y_test)\n",
    "y_test_inverse = scaler_flow.inverse_transform(y_test)\n",
    "\n",
    "# Display predictions\n",
    "results = pd.DataFrame({'Actual Flow': y_test_inverse.flatten(), 'Predicted Flow': predicted_flows.flatten()})\n",
    "print(results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_test_inverse.flatten(), predicted_flows.flatten())\n",
    "mape = mean_absolute_percentage_error(y_test_inverse.flatten(), predicted_flows.flatten())\n",
    "r2 = r2_score(y_test_inverse.flatten(), predicted_flows.flatten())\n",
    "print(f'MAE: {mae}, MAPE: {mape}, R^2: {r2}')\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Model Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rainfall-flow dual-axis chart for actual vs predicted flows along with aggregated rainfall\n",
    "fig, ax1 = plt.subplots(figsize=(18, 10))\n",
    "draw_len = len(y_test_inverse)\n",
    "\n",
    "# Use first 100 data points for plotting\n",
    "time_steps = np.arange(draw_len)\n",
    "# aggregated_rainfall = scaler_rainfall.inverse_transform(X_test[:draw_len, :, :-1].reshape(-1, X_test.shape[2] - 1)).sum(axis=1).reshape(draw_len, -1).mean(axis=1)\n",
    "aggregated_rainfall = scaler_rainfall.inverse_transform(X_test[:draw_len, :, :-1].reshape(-1, X_test.shape[2] - 1)).sum(axis=1).reshape(draw_len, -1)[:,0]\n",
    "\n",
    "# Plot aggregated rainfall as a bar chart, pointing downwards\n",
    "ax1.bar(time_steps, aggregated_rainfall, color='tab:green', alpha=0.5, label='Aggregated Rainfall')\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_xlabel('Time Steps')\n",
    "ax1.set_ylabel('Aggregated Rainfall (mm)')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.set_title('Rainfall and Flow Relationship')\n",
    "\n",
    "# Apply smoothing to actual and predicted flow\n",
    "actual_flow = y_test_inverse[:draw_len, 0]\n",
    "predicted_flow = predicted_flows[:draw_len, 0]\n",
    "\n",
    "# Plot actual and predicted flow\n",
    "ax2 = ax1.twinx() \n",
    "ax2.plot(time_steps, actual_flow, label='Actual Flow', color='blue', alpha=0.7)\n",
    "ax2.plot(time_steps, predicted_flow, label='Predicted Flow', color='red', linestyle='dashed', alpha=0.7)\n",
    "\n",
    "ax2.set_ylabel('Flow (m^3/s)')\n",
    "ax2.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted flows 全部时间步\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(y_test_inverse.flatten(), label='Actual Flow')\n",
    "plt.plot(predicted_flows.flatten(), label='Predicted Flow')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Flow')\n",
    "plt.legend()\n",
    "plt.title('Actual vs Predicted Flow')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18 (default, Sep 11 2023, 08:28:20) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5e36ac6a21e329c2cec267b08e4f28884519c7e5682f29504bd17199cc3d203"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
