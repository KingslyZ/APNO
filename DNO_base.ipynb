{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepxde as dde\n",
    "import numpy as np\n",
    "from deepxde.backend import tf \n",
    "from scipy import io\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 读取 CSV 文件（假设文件有列标题）\n",
    "df = pd.read_csv('./changhua_data.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 2. 提取输入特征和目标列\n",
    "# 这里假设第2列是输入特征，第1列是目标值\n",
    "col_x = df.iloc[:, 2].values.reshape(-1, 1)[0:100]  # 第2列 (输入特征)\n",
    "col_y = df.iloc[:, 1].values.reshape(-1, 1)[0:100]  # 第1列 (目标)\n",
    "\n",
    "# 3. 划分训练集和测试集，80%训练，20%测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(col_x, col_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. 数据标准化（如果需要标准化）\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler_x.fit_transform(X_train)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "\n",
    "X_test_scaled = scaler_x.transform(X_test)\n",
    "y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "# 5. 创建 DeepXDE 的 DataSet\n",
    "data = dde.data.DataSet(\n",
    "    X_train=X_train_scaled,\n",
    "    y_train=y_train_scaled,\n",
    "    X_test=X_test_scaled,\n",
    "    y_test=y_test_scaled\n",
    ")\n",
    "\n",
    "# 6. 检查数据\n",
    "print(\"训练数据输入 X_train:\", X_train_scaled[:5])\n",
    "print(\"训练数据输出 y_train:\", y_train_scaled[:5])\n",
    "print(\"测试数据输入 X_test:\", X_test_scaled[:5])\n",
    "print(\"测试数据输出 y_test:\", y_test_scaled[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size = [1] + [50] * 3 + [1]\n",
    "activation = \"tanh\"\n",
    "initializer = \"Glorot normal\"\n",
    "net = dde.nn.FNN(layer_size, activation, initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dde.Model(data, net)\n",
    "model.compile(\"adam\", lr=0.001, metrics=[\"l2 relative error\"])\n",
    "losshistory, train_state = model.train(iterations=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dde.saveplot(losshistory, train_state, issave=True, isplot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 读取CSV文件\n",
    "df = pd.read_csv(\"./changhua_data.csv\")\n",
    "\n",
    "# 2. 提取并处理时间列 (第1列)\n",
    "# 使用 pandas 将时间转换为 datetime 对象\n",
    "df['time'] = pd.to_datetime(df.iloc[:, 0])\n",
    "\n",
    "# 将时间转换为数值形式，例如时间戳（秒）\n",
    "x = df['time'].astype(np.int64) // 10**9  # 转换为秒级时间戳\n",
    "x = x.values.reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "# 3. 分支网络输入：气象站的降雨数据 (第3到第9列)\n",
    "v = df.iloc[:, 2:9].values.astype(np.float32)\n",
    "\n",
    "# 4. 目标：流量 (第2列)\n",
    "y = df.iloc[:, 1].values.reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "# 5. 使用 train_test_split 划分数据集为 80% 训练集和 20% 测试集\n",
    "X_train_branch, X_test_branch, X_train_trunk, X_test_trunk, y_train, y_test = train_test_split(\n",
    "    v, x, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 6. 构建X_train为一个元组，分别对应分支和主干网络的输入\n",
    "X_train = (X_train_branch, X_train_trunk)\n",
    "X_test = (X_test_branch, X_test_trunk)\n",
    "print(X_train_branch.shape)\n",
    "print(X_train_trunk.shape)\n",
    "print(y_train.shape)\n",
    "# len(X_test[1]) != y_test.shape[1]:\n",
    "print(y_test.shape[1])\n",
    "print(len(X_test[1]))\n",
    "# X_train[0]) != y_train.shape[0]\n",
    "print(len(X_train[1]))\n",
    "print(y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dde.data.TripleCartesianProd(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test, y_test=y_test  # 假设你有测试数据\n",
    ")\n",
    "\n",
    "# 7. 现在你可以将这些数据用于 DeepONet 的输入\n",
    "\n",
    "# 然后定义网络并训练\n",
    "m = 100  # 假设有100个气象站输入\n",
    "dim_x = 1  # 时间作为输入维度\n",
    "net = dde.nn.DeepONetCartesianProd(\n",
    "    [m, 40, 40],\n",
    "    [dim_x, 40, 40],\n",
    "    \"relu\",\n",
    "    \"Glorot normal\",\n",
    ")\n",
    "\n",
    "model = dde.Model(data, net)\n",
    "model.compile(\"adam\", lr=0.001, metrics=[\"mean l2 relative error\"])\n",
    "losshistory, train_state = model.train(iterations=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "d = np.load(\"/Users/zjr/Downloads/antiderivative_aligned_train.npz\", allow_pickle=True)\n",
    "X_train = (d[\"X\"][0].astype(np.float32), d[\"X\"][1].astype(np.float32))\n",
    "y_train = d[\"y\"].astype(np.float32)\n",
    "d = np.load(\"/Users/zjr/Downloads/antiderivative_aligned_test.npz\", allow_pickle=True)\n",
    "X_test = (d[\"X\"][0].astype(np.float32), d[\"X\"][1].astype(np.float32))\n",
    "y_test = d[\"y\"].astype(np.float32)\n",
    "\n",
    "data = dde.data.TripleCartesianProd( \n",
    "    X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test\n",
    ")\n",
    "\n",
    "# Choose a network\n",
    "m = 100\n",
    "dim_x = 1\n",
    "net = dde.nn.DeepONetCartesianProd(\n",
    "    [m, 40, 40],\n",
    "    [dim_x, 40, 40],\n",
    "    \"relu\",\n",
    "    \"Glorot normal\",\n",
    ")\n",
    "\n",
    "# Define a Model\n",
    "model = dde.Model(data, net)\n",
    "\n",
    "# Compile and Train\n",
    "model.compile(\"adam\", lr=0.001, metrics=[\"mean l2 relative error\"])\n",
    "losshistory, train_state = model.train(iterations=10000)\n",
    "\n",
    "# Plot the loss trajectory\n",
    "dde.utils.plot_loss_history(losshistory)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "file_path=\"/Users/zjr/Downloads/antiderivative_aligned_test.npz\"\n",
    "poem=np.load(file_path,allow_pickle=True)\n",
    "poem.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = poem['X'],poem['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[0].shape)\n",
    "print(x[1].shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import deepxde as dde\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./changhua_data.csv')\n",
    "\n",
    "# Extract columns\n",
    "# Assume: First column is time, second column is flow, third to ninth are rainfall data\n",
    "times = pd.to_datetime(data.iloc[:, 0].values)  # Convert time to datetime\n",
    "flows = data.iloc[:, 1].values.reshape(-1, 1)\n",
    "rainfalls = data.iloc[:, 2:].values\n",
    "\n",
    "# Create sequences for time-series prediction\n",
    "def create_sequences(data, target, input_steps, output_steps):\n",
    "    X, y, times_seq = [], [], []\n",
    "    for i in range(len(data) - input_steps - output_steps + 1):\n",
    "        X.append(data[i:(i + input_steps), :])\n",
    "        y.append(target[(i + input_steps):(i + input_steps + output_steps), 0])\n",
    "        times_seq.append(times[i + input_steps:(i + input_steps + output_steps)])\n",
    "    return np.array(X), np.array(y), np.array(times_seq)\n",
    "\n",
    "input_steps = 12  # Use past 12 hours\n",
    "output_steps = 6  # Predict next 6 hours\n",
    "\n",
    "# Data preprocessing\n",
    "scaler_flow = MinMaxScaler()\n",
    "flows_scaled = scaler_flow.fit_transform(flows)\n",
    "\n",
    "scaler_rainfall = MinMaxScaler()\n",
    "rainfalls_scaled = scaler_rainfall.fit_transform(rainfalls)\n",
    "\n",
    "# Combine rainfall and flow data for sequence creation\n",
    "combined_data = np.concatenate((rainfalls_scaled, flows_scaled), axis=1)\n",
    "\n",
    "# Create sequences\n",
    "X, y, times_seq = create_sequences(combined_data, flows_scaled, input_steps, output_steps)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test, times_train, times_test = train_test_split(X, y, times_seq, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define DeepONet model using DeepXDE\n",
    "# branch = dde.maps.FNN([input_steps * (X_train.shape[2] - 1), 64, 32], \"relu\", \"Glorot normal\")\n",
    "# trunk = dde.maps.FNN([input_steps, 64, 32], \"relu\", \"Glorot normal\")\n",
    "# net = dde.maps.DeepONet(branch, trunk)\n",
    "\n",
    "branch = dde.maps.FNN([input_steps * (X_train.shape[2] - 1), 64, 32], \"relu\", \"Glorot normal\")\n",
    "trunk = dde.maps.FNN([input_steps, 64, 32], \"relu\", \"Glorot normal\")\n",
    "net = dde.maps.DeepONet(branch, trunk, activation=\"relu\", kernel_initializer=\"Glorot normal\")\n",
    "\n",
    "model = dde.Model(data=None, net=net)\n",
    "model.compile(\"adam\", lr=0.001, metrics=[\"mae\"])\n",
    "\n",
    "# Prepare data for DeepONet\n",
    "branch_train = X_train[:, :, :-1].reshape(len(X_train), -1)\n",
    "trunk_train = X_train[:, :, -1:].reshape(len(X_train), -1)\n",
    "y_train_flat = y_train.reshape(-1, output_steps)\n",
    "\n",
    "branch_test = X_test[:, :, :-1].reshape(len(X_test), -1)\n",
    "trunk_test = X_test[:, :, -1:].reshape(len(X_test), -1)\n",
    "y_test_flat = y_test.reshape(-1, output_steps)\n",
    "\n",
    "# Train the model\n",
    "losshistory, train_state = model.train([branch_train, trunk_train], y_train_flat, epochs=100, batch_size=16, validation_data=([branch_test, trunk_test], y_test_flat))\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation = model.evaluate([branch_test, trunk_test], y_test_flat)\n",
    "print(f'Test Loss: {evaluation[0]}, Test MAE: {evaluation[1]}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict([branch_test, trunk_test])\n",
    "\n",
    "# Inverse transform the predicted flows to original scale\n",
    "predicted_flows = scaler_flow.inverse_transform(predictions.reshape(-1, 1)).reshape(-1, output_steps)\n",
    "y_test_inverse = scaler_flow.inverse_transform(y_test.reshape(-1, 1)).reshape(-1, output_steps)\n",
    "\n",
    "# Display predictions\n",
    "results = pd.DataFrame({'Actual Flow': y_test_inverse.flatten(), 'Predicted Flow': predicted_flows.flatten()})\n",
    "print(results.head())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_test_inverse.flatten(), predicted_flows.flatten())\n",
    "mape = mean_absolute_percentage_error(y_test_inverse.flatten(), predicted_flows.flatten())\n",
    "r2 = r2_score(y_test_inverse.flatten(), predicted_flows.flatten())\n",
    "print(f'MAE: {mae}, MAPE: {mape}, R^2: {r2}')\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losshistory.loss_train, label='Training Loss')\n",
    "plt.plot(losshistory.loss_test, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Model Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot rainfall-flow dual-axis chart for actual vs predicted flows along with aggregated rainfall\n",
    "fig, ax1 = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Use first 100 data points for plotting\n",
    "time_steps = np.arange(100)\n",
    "aggregated_rainfall = scaler_rainfall.inverse_transform(X_test[:100, :, :-1].reshape(-1, X_test.shape[2] - 1)).sum(axis=1).reshape(100, -1).mean(axis=1)\n",
    "\n",
    "# Plot aggregated rainfall as a bar chart, pointing downwards\n",
    "ax1.bar(time_steps, -aggregated_rainfall, color='tab:green', alpha=0.5, label='Aggregated Rainfall')\n",
    "ax1.invert_yaxis()\n",
    "\n",
    "ax1.set_xlabel('Time Steps')\n",
    "ax1.set_ylabel('Aggregated Rainfall (mm)')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.set_title('Rainfall and Flow Relationship')\n",
    "\n",
    "# Plot actual and predicted flow\n",
    "actual_flow = y_test_inverse[:100, 0]\n",
    "predicted_flow = predicted_flows[:100, 0]\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(time_steps, actual_flow, label='Actual Flow', color='blue', alpha=0.7)\n",
    "ax2.plot(time_steps, predicted_flow, label='Predicted Flow', color='red', linestyle='dashed', alpha=0.7)\n",
    "\n",
    "ax2.set_ylabel('Flow (m^3/s)')\n",
    "ax2.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18 (default, Sep 11 2023, 08:28:20) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5e36ac6a21e329c2cec267b08e4f28884519c7e5682f29504bd17199cc3d203"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
